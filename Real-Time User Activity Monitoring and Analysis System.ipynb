{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5852d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UserActivityMonitoring\").getOrCreate()\n",
    "\n",
    "# Define schema for incoming data\n",
    "schema = StructType() \\\n",
    "    .add(\"user_id\", StringType()) \\\n",
    "    .add(\"event_type\", StringType()) \\\n",
    "    .add(\"page\", StringType()) \\\n",
    "    .add(\"timestamp\", TimestampType())\n",
    "\n",
    "# Read from Kafka or socket\n",
    "activity_df = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7152bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Active users in the last 5 minutes\n",
    "active_users = activity_df \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"user_id\") \\\n",
    "    .count()\n",
    "\n",
    "# Most visited pages in the last 10 minutes\n",
    "page_views = activity_df \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"10 minutes\"), \"page\") \\\n",
    "    .count() #\\\n",
    "    #.orderBy(\"count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8fb772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x26a754c32e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output active users to console\n",
    "active_users.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Output page views to console\n",
    "page_views.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b688ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = spark.read.csv(\"./dataset/airports.csv\", header=True, schema=schema)\n",
    "\n",
    "# Total unique users\n",
    "total_users = historical_df.select(\"user_id\").distinct().count()\n",
    "\n",
    "# Popular pages\n",
    "popular_pages = historical_df.groupBy(\"page\").count().orderBy(\"count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4445543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|page|views|\n",
      "+----+-----+\n",
      "|  CA|   32|\n",
      "|  TX|   29|\n",
      "|  AK|   22|\n",
      "|  FL|   20|\n",
      "|  NY|   14|\n",
      "|  MI|   12|\n",
      "|  NC|   11|\n",
      "|  CO|   11|\n",
      "|  GA|    8|\n",
      "|  WI|    8|\n",
      "|  IL|    8|\n",
      "|  MT|    8|\n",
      "|  OR|    7|\n",
      "|  PA|    7|\n",
      "|  MO|    7|\n",
      "|  WA|    7|\n",
      "|  LA|    7|\n",
      "|  MS|    7|\n",
      "|  HI|    7|\n",
      "|  OH|    6|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "historical_df.createOrReplaceTempView(\"activity\")\n",
    "\n",
    "# Query for specific insights\n",
    "popular_pages_sql = spark.sql(\"SELECT page, COUNT(*) AS views FROM activity GROUP BY page ORDER BY views DESC\")\n",
    "popular_pages_sql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a43d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concepts Applied\n",
    "#Structured Streaming: For real-time user activity monitoring.\n",
    "#Batch Processing: For analyzing historical data.\n",
    "#Spark SQL: For querying data and aggregations.\n",
    "#DataFrame and RDD Operations: Used to load and transform data.\n",
    "#Window Operations: To group data by time intervals.\n",
    "#File Systems: For storing historical data and intermediate outputs.\n",
    "#Fault Tolerance and Checkpointing: Ensuring resilience in the streaming pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
